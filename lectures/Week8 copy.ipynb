{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "It's the last time we meet in class for exercises! And to celebrate this mile-stone, we've put together an amazing set of exercises. And if you're behind, don't worry. The workload is low!\n",
    "\n",
    "  - Part A: First, we play around with sentiment analysis\n",
    "  - Part B **(optional)**: We study paths in networks using a fun new dataset. (If only doing part A feels too easy for you, I've added a really fun exercise that should be fun and challenging.)\n",
    "\n",
    "But first, watch this video that touches upon your final project and Assignment 2 (which will be released later Wednesday),\n",
    "\n",
    "> [**Video**](https://www.dropbox.com/scl/fi/jh3490oqnqh4yl629ik09/ProjectAndAssigment.mp4?rlkey=2uyl7tq0xgzydl5u8pqzncf92&dl=0) about the Independent Project and Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is another highly useful technique which we'll use to make sense of the Wiki\n",
    "data. Further, experience shows that it might well be very useful when you get to the project stage of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> [**Video Lecture**](https://www.dropbox.com/scl/fi/u0rn767hg7e6ue4ryx41a/Sentiment.mp4?rlkey=4kwfqv6pkw1aijshm5zqe8ehh&dl=0): Sentiment and dictionary-based methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reading: [Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 1_: Sentiment distribution. \n",
    "> \n",
    "> * Download the LabMT wordlist. It's available as supplementary material from [Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752) (Data Set S1). Describe briefly how the list was generated.\n",
    "> * Based on the LabMT word list, write a function that calculates sentiment given a list of tokens (the tokens should be lower case, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Defining sentiment data file path\n",
    "pwd = os.getcwd()\n",
    "filepath = os.path.join(pwd, \"dataset_s1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.39"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting words and sentiment scores from the data into a dictionary\n",
    "word_sentiment_dict = {}\n",
    "with open(filepath, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Skipping the header line\n",
    "for line in lines[1:]:  \n",
    "    \n",
    "    # Reading separate columns split by tab character\n",
    "    tokens = line.split(\"\\t\")\n",
    "\n",
    "    # Reading the word and corresponding sentiment score into the dict\n",
    "    word = tokens[0]  \n",
    "    sentiment = float(tokens[2])\n",
    "    word_sentiment_dict[word] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(tokens):\n",
    "    \"\"\"\n",
    "    Calculate sentiment score based on the list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - tokens (list): A list of words.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The average sentiment score of the tokens.\n",
    "    \"\"\"\n",
    "    total_sentiment = 0\n",
    "    count = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in word_sentiment_dict:\n",
    "            total_sentiment += word_sentiment_dict[token]\n",
    "            count += 1\n",
    "    \n",
    "    # Return the average sentiment if count is not zero; otherwise, return None\n",
    "    return total_sentiment / count if count != 0 else None\n",
    "\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function to remove stopwords, lemmatize, lowercase and get sentiment for a text\n",
    "def get_sentiment_for_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filter out stopwords and non-alphabetic tokens\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
    "    \n",
    "    # Lemmatize and lowercase tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in filtered_tokens]\n",
    "    \n",
    "    # Calculate sentiment\n",
    "    sentiment = calculate_sentiment(lemmatized_tokens)\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('6ix9ine.txt', 5.455729001584776),\n",
       " ('9th_Prince.txt', 5.457936507936506),\n",
       " ('22Gz.txt', 5.5451),\n",
       " ('38_Spesh.txt', 5.690217391304354),\n",
       " ('The_45_King.txt', 5.619158751696069)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get each rapper file by the rapper name\n",
    "# URL of the CSV file\n",
    "rappers_url = 'https://raw.githubusercontent.com/SocialComplexityLab/socialgraphs2023/main/files/Rappers.csv'\n",
    "\n",
    "# Reading the CSV file into a pandas DataFrame\n",
    "df_rappers = pd.read_csv(rappers_url)\n",
    "rapper_file_root = 'rappers_text/'\n",
    "file_list = [f\"{re.sub(r'[^a-zA-Z0-9_]', '_', rapper_name)}.txt\" for rapper_name in df_rappers['WikipediaPageName']]\n",
    "\n",
    "# Dictionary to store sentiment values for each rapper\n",
    "rapper_sentiments = {}\n",
    "\n",
    "# Iterate over each file and calculate sentiment\n",
    "for rapper_file_name in file_list:\n",
    "    with open(os.path.join(rapper_file_root, rapper_file_name), 'r', encoding='utf-8') as file:\n",
    "        # Read file text to a string\n",
    "        content = file.read()\n",
    "        # Calculate the sentiment for the file text\n",
    "        sentiment = get_sentiment_for_text(content)\n",
    "        \n",
    "        rapper_sentiments[rapper_file_name] = sentiment\n",
    "\n",
    "# Checking the first few sentiment values\n",
    "list(rapper_sentiments.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Iterate over the nodes in your network, tokenize each page, and calculate sentiment every single page. Now you have sentiment as a new nodal property. \n",
    "> * Remember histograms? Create a histogram of all character's associated page-sentiments. (And make it a nice histogram - use your histogram making skills from Week 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617569"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import the tokenized text for east and west coast\n",
    "# Step 1\n",
    "# tokenized_east_coast_texts = word_tokenize(' '.join(rappers_texts_by_coast['East']))\n",
    "with open('tokenized_east_coast_texts.pkl', 'rb') as f:\n",
    "    tokenized_east_coast_texts = pkl.load(f)\n",
    "\n",
    "# tokenized_west_coast_texts = word_tokenize(' '.join(rappers_texts_by_coast['West']))\n",
    "with open('tokenized_west_coast_texts.pkl', 'rb') as f:\n",
    "    tokenized_west_coast_texts = pkl.load(f)\n",
    "\n",
    "filtered_tokenized_east_coast_texts = [token for token in tokenized_east_coast_texts if token not in string.punctuation]\n",
    "filtered_tokenized_west_coast_texts = [token for token in tokenized_west_coast_texts if token not in string.punctuation]\n",
    "\n",
    "# Lowercase everything\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lowercase and lemmatize\n",
    "lemmatized_east_coast_tokens = [lemmatizer.lemmatize(token.lower()) for token in filtered_tokenized_east_coast_texts]\n",
    "lemmatized_west_coast_tokens = [lemmatizer.lemmatize(token.lower()) for token in filtered_tokenized_west_coast_texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Who are the 10 rappers with happiest and saddest pages?\n",
    "\n",
    "<!---\n",
    "> * Now we average the average sentiment of the nodes in each community to find a *community level sentiment*. \n",
    ">   - Name each community by its three most connected characters. \n",
    ">   - What are the three happiest communities? \n",
    ">   - what are the three saddest communities?\n",
    ">   - Do these results confirm what you can learn about each community by skimming the wikipedia pages?\n",
    "--->\n",
    "\n",
    "**Note**: Calculating sentiment takes a long time, so arm yourself with patience as your code runs (remember to check that it runs correctly, before waiting patiently). Further, these tips may speed things up. And save somewhere, so you don't have to start over.\n",
    "\n",
    "**Tips for speed**\n",
    "* If you use `freqDist` prior to finding the sentiment, you only have to find it for every unique word and hereafter you can do a weighted mean.\n",
    "* More tips for speeding up loops https://wiki.python.org/moin/PythonSpeed/PerformanceTips#Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 2*: West coast and East coast.\n",
    "\n",
    "Now, let's check if the pages of rappers from different coasts tend to have different sentiment. \n",
    "\n",
    "> * First, think about it for a second: Do you expect pages of rappers from different coasts to have different sentiment? Why/why not?\n",
    "> * Next, Download [this file](https://github.com/SocialComplexityLab/socialgraphs2023/edit/main/files/Rappers.csv) if you do not have it already.\n",
    "> * Start by plotting the distribution of sentiment for the pages of east-coast and west-coast rappers separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Now calculate the mean, median, 10th percentile, and 90 percentile sentiment value for each coast and indicate those values on the corresponding histograms.\n",
    "> * Comment on these distributions and how they relate to the overall sentiment distribution you calculated in Exercise 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Use the \"label shuffling technique\" that I illustrated in **Week 5, exercise 7** to determine whether or not \n",
    ">   1. the west-coast pages tend to have lower sentiment than a similarly sized group of randomly chosen rapper pages (pick random groups 1000 times and reject the hypothesis if 5% of the random outcomes are lower than the actual value observed for the west-coast pages)\n",
    ">   2. the west-coast pages tend to have higher sentiment than a similarly sized group of randomly chosen character pages (again, use 1000 random draws and a 5% confidence bound).\n",
    ">   3. (**optional**) Perhaps there are other divisions of rappers whose pages would be more likely to have different sentiment. Perhaps comparing groups of rappers of different sexes, races, etc. would give interesting results? Feel free to do a deep dive here, and let me know which groups you analyze and how you end up \"guessing\" a rapper's sex, race, etc. \n",
    "\n",
    "Comment: The \"label shuffling technique\" is incredibly useful. It may turn out to be a good tool to apply for your independent project. Keep it in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B (optional): The Wikispeedia Extravaganza\n",
    "\n",
    "This final **AND OPTIONAL** exercise is about tying everything together. In this context \"optional\" means that the exercises below will not be on the assignment (but they're fun & cool).\n",
    "\n",
    "We try to combine our work on networks plus our work on language in order to understand and analyze human behavior. We'll be working on a cool dataset of humans playing a game on (a special version of) wikipedia called \"wikispeedia\".  Note that the game is now called *The Wiki Game* and can be found at http://thewikigame.com/ ( ... while \"http://wikispeedia.org\" is a page about GPS coordinates of speed-traps). \n",
    "\n",
    "Here's how the game worked:\n",
    "\n",
    "> In the game, users are asked to navigate from a given _source_ article (e.g. https://en.wikipedia.org/wiki/Gold_dollar) to a given _target_ article (e.g. https://en.wikipedia.org/wiki/Ronald_Reagan), **by only clicking Wikipedia links**. A condensed version of Wikipedia (4,604 articles) is used. \n",
    "\n",
    "So this dataset contains **human navigation paths** (clicking from page to page to find a target) and today we will work on this dataset to see if we can use our skills to understand how human navigation works. We will think about the following questions.\n",
    "\n",
    "* Path lengths\n",
    "* Betweenness from the human perspective\n",
    "* What characterizes human paths?\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "The first thing we're going to do is download the dataset. Today everything related to data is nice, clean, and easy to work with (Yay). You can get the dataset [here](https://snap.stanford.edu/data/wikispeedia.html). You will need to get \n",
    "\n",
    "* The list of wiki articles\n",
    "* The network connections\n",
    "* The navigation paths\n",
    "* Plaintext of the wiki articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path lengths\n",
    "\n",
    "The first thing we want to take a look at is path lengths. NetworkX allows us to calculate the shortest path between any pair of articles. We begin by comparing the length of human and shortests paths. \n",
    "\n",
    "_Optional Exercises 1_\n",
    "> * For each _source_/_target_ pair in the list of human navigation paths, calculate the shortest path using NetworkX. Plot the distribution of path lengths. Mine looks something like this (if I use an undirected graph):\n",
    "![alt text](https://raw.githubusercontent.com/suneman/socialgraphs2016/master/files/shortest-path.png)\n",
    "\n",
    "\n",
    "> * For each _source_/_target_ pair, calculate the length of the human path. The dataset contains information on people who regret a navigation step and hit the \"back\" button in their web-browser. It's up to you how to incorporate that information in the path. Justify your choice. Plot the distribution of human path lengths. If I ignore back steps, I get this on log-log scale:\n",
    "![alt text](https://raw.githubusercontent.com/suneman/socialgraphs2016/master/files/human-path.png)\n",
    "\n",
    "> * How much longer are the human paths on average?\n",
    "> * Create scatter plot where each point is a _source_/_target_ pair, and you have human path lengths on the $x$-axis and shortests paths on the $y$-axis.\n",
    "> * Is there a correlation between human/shortest path-lengths? What is the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betweenness\n",
    "\n",
    "An interesting definition of centrality is _betweenness centrality_ (here's a handy [link to the NetworkX documentation](http://networkx.readthedocs.io/en/stable/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html)). In a traditional setting, this measure calculates all shortest paths in the network and then each node gets a score according to which fraction of all shortest paths pass through that node.\n",
    "\n",
    "\n",
    "In this part, we will create our own version of centrality, based on the _source_/_target_ pairs in our dataset. We define a node's **navigation centrality** as follows. \n",
    "\n",
    "> *Navigation centrality* of node $i$ is the fraction of all navigation paths that pass through $i$. We exclude the source and target from the count. If a node has not been visited by a search, the navigation centrality of that node is defined to be zero.\n",
    "\n",
    "In the exercises below, we investigate the relationship between navigation centrality and betweenness centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional Exercises 2*\n",
    "\n",
    "> Begin by calculating the betweenness centrality and navigation centrality of all nodes in the Wikispeedia dataset.\n",
    "> Note that calculating the betweenness centrality can take quite a long time, so you might start it running in a separate notebook while first estimating it based on the existing human path.\n",
    ">\n",
    "> * First, list the 5 pages with highest navigation centrality.\n",
    "> * Second, list the 5 pages with highest betweenness centrality.\n",
    "> * Compare the two lists. Explain the differences between the two lists in your own words.\n",
    "> * Create a scatterplot of betweenness centrality vs. navigation centrality.\n",
    "> * Let's explore the pages that have navigation centrality equal to zero.\n",
    ">   * How many pages have zero navigation centrality?\n",
    ">   * What is the the page with zero navigation centrality and highest betweenness centrality? Can you explain why no human navigated to this page? Can you explain why the page is central in the actual link network? (For example, you can take a look at the degree of the node).\n",
    ">   * Plot the distribution of betweenness centrality for the pages with zero navigation centrality. My plot on log-log scale:\n",
    "![alt text](https://raw.githubusercontent.com/suneman/socialgraphs2016/master/files/betweenness.png)\n",
    "\n",
    "> * Now, let's *throw out all pages with zero navigation centrality* and compare navigation- and betweenness centrality for the remaining pages.\n",
    ">   * What is the correlation between betweenness centrality and navigation centrality?\n",
    ">   * Comment on the top 5 outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing the text into the picture\n",
    "\n",
    "Now that we have an idea about the differences between how humans and computers search in networks, we are going to dig a little deeper using the page content to test a hypothesis to explain why the human navigation paths are longer. The general idea is that humans (who don't know about the global network structure) tend to jump between pages that have related _content_. For this reason we expect that (on average) human navigation paths have more similar content than the shortest paths in the network (which might take 'surprising' shortcuts via relatively unrelated pages). In short.\n",
    "\n",
    "> **Hypothesis H1**: Human navigation paths have more similar content than network shortest paths.\n",
    "\n",
    "The way we'll test this hypothesis is to first represent each page as a vector using a bag-of-words approach, then we can calculate a distance between pairs of pages using some vector-space difference, and finally we'll characterize each path by its average pair-wise distance. Below, I've set up that process as an exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional Exercises 3*\n",
    "\n",
    "> First, create a TF-IDF vector for each page. You already know all about TF-IDF from last week's exercise. The main difference is that we now _characterize **each page** by a TF-IDF vector_ and not a group of pages.\n",
    "> \n",
    "> Second, write a function that calculates the distance between a pair of vectors. There are many ways to calculate distances between a pair of vectors (try a Google search for `vector space distance measures` if you want to refresh your knowledge on this topic). You're free to choose what you want, but we recommend the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    ">\n",
    "> Now you're ready for the exercises\n",
    "> \n",
    "> * Calculate the average pairwise similarity for all human navigation paths (the _source_/_target_ pairs from above). With start and end at node $i,j$ we can call this similarity $s_{i,j}$. Calculate mean/variance of the $s_{ij}$'s.\n",
    "> * Calculate the average pairwise similarity for all shortest paths between the _source_/_target_ pairs ($S_{i,j}$). Calculate mean/variance of the $S_{i,j}$.\n",
    "> * Plot the distributions of average similarities for both human- and shortest paths in a single plot. If everything works well, you should see something similar to the following:\n",
    "![alt text](https://raw.githubusercontent.com/suneman/socialgraphs2016/master/files/path-similarity.png)\n",
    "\n",
    "> * Finally, for each source/target pair, compare the human-navigation average similarity with the betweenness based average similarity, testing what fraction of the time, the average similarity is lower in the case of human navigation.\n",
    "> * Comment on your findings. Is **H1** true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
