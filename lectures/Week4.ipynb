{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This week is all about working with data. I'm not going to lie to you. This part might be frustrating - but frustration is an integral part of learning. Real data is almost always messy & difficult ... and learning to deal with that fact, is a key part of being a data scientist. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough about the process, let's get to the content. \n",
    "\n",
    "![Text](https://wallpapers.com/images/high/cool-rapper-zs7xat10uqylszmy.webp \"Great image choice, Jonas\")\n",
    "\n",
    "Today, we will use network science and Wikipedia to learn about the relationships of **[West Coast](https://en.wikipedia.org/wiki/Category:West_Coast_hip_hop_musicians)** and **[East coast](https://en.wikipedia.org/wiki/Category:East_Coast_hip_hop_musicians)** rappers. \n",
    "\n",
    "To create the network, we will download the Wikipedia pages for all rappers from each coast. Next, we will create the network of the pages that link to each other. Since wikipedia pages link to each other. So [Snoop Dogg](https://en.wikipedia.org/wiki/Snoop_Dogg) links to [Dr. Dre](https://en.wikipedia.org/wiki/Dr._Dre), for example.\n",
    "\n",
    "Next time, we'll use our network skills (as well as new ones) to understand that network. Further down the line, we'll use natural language processing to understand the text displayed on those pages.\n",
    "\n",
    "But for today, the tasks are\n",
    "\n",
    "* Learn about regular expressions\n",
    "* Learn about Pandas dataframes\n",
    "* Download and store (for later use) all the rapper-pages from Wikipedia\n",
    "* Extract all the internal wikipedia-links that connect the rappers on wikipedia\n",
    "* Generate the network of rappers on wikipedia. \n",
    "* Calculate some simple network statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Guide to Week 4 (not to be missed)\n",
    "\n",
    "Today I talk about \n",
    "\n",
    "* Results of the user satisfaction questionnaire\n",
    "* Assignment 1\n",
    "* Today's exercises\n",
    "\n",
    "> * ***Video lecture*** Guide to week 4 https://www.dropbox.com/scl/fi/b760tkugfrnm9kca1apnb/GuideToWeek4.mp4?rlkey=r7y6pijkafc9zn5tcz1cmj8dg&dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prelude: Regular expressions\n",
    "\n",
    "Before we get started, we have to get a little head start on the _Natural Language Processing_ part of the class. This is a new direction for us, up to now, we've mostly been doing math-y stuff with Python, but today, we're going to be using Python to work through a text. The central thing we need to be able to do today, is to extract internal wikipedia links. And for that we need regular expressions.\n",
    "\n",
    "> _Exercises_: Regular expressions round 1\\.\n",
    "> \n",
    "> * Read [**this tutorial**](https://developers.google.com/edu/python/regular-expressions) to form an overview of regular expressions. This is important to understand the content of the tutorial (also very useful later), so you may actually want to work through the examples.\n",
    "> * Now, explain in your own words: what are regular expressions?\n",
    "> * Provide an example of a regex to match 4 digits numbers (by this, I mean precisely 4 digits, you should not match any part of numbers with e.g. 5 digits). In your notebook, use `findall` to show that your regex works on this [test-text](https://raw.githubusercontent.com/SocialComplexityLab/socialgraphs2020/master/files/regex_exercise.txt). **Hint**: a great place to test out regular expressions is: https://regex101.com.\n",
    "> * Provide an example of a regex to match words starting with \"super\". Show that it works on the [test-text](https://raw.githubusercontent.com/SocialComplexityLab/socialgraphs2020/master/files/regex_exercise.txt).\n",
    "> \n",
    "\n",
    "Finally, we need to figure out how how to match internal wiki links. Wiki links come in two flavors. They're always enclosed in double square brackets, e.g. `[[wiki-link]]` and can either occur like this:\n",
    "\n",
    "    ... some text [[Aristotle]] some more text ...\n",
    "\n",
    "which links to the page [`https://en.wikipedia.org/wiki/Aristotle`](https://en.wikipedia.org/wiki/Aristotle). \n",
    "\n",
    "The second flavor has two parts, so that links can handle spaces and other more fancy forms of references, here's an example:\n",
    "\n",
    "    ... some text [[John_McCain|John McCain]] some more text ...\n",
    "\n",
    "which links to the page [`https://en.wikipedia.org/wiki/John_McCain`](https://en.wikipedia.org/wiki/Eudemus_of_Rhodes). Now it's your turn.\n",
    "\n",
    "> _Exercise_: Regular expressions round 2\\. Show that you can extract the wiki-links from the [test-text](https://raw.githubusercontent.com/SocialComplexityLab/socialgraphs2020/master/files/regex_exercise.txt). Perhaps you can find inspiration on stack overflow or similar. **Hint**: Try to solve this exercise on your own (that's what you will get the most out of - learning wise), but if you get stuck ... you will find the solution in one of the video lectures below.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude part 2: Pandas DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we will also learn a bit about [pandas dataframes](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), a very user-friendly data structure that you can use to manipulate tabular data. Pandas dataframes are implemented within the [pandas package] (https://pandas.pydata.org/).\n",
    "\n",
    "Pandas dataframes should be intuitive to use. **We suggest you to go through the [10 minutes to Pandas tutorial](https://pandas.pydata.org/pandas-docs/version/0.22/10min.html#min) to learn what you need to solve the next exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part A: Download the Wikipedia pages of rappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to download all of the pages of the characters. Use your experience with APIs from Week 1\\. To get started, I **strongly** recommend that you revisit the [**APIs note book**](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/API_check.ipynb) from that week - it contains lots of useful tips on this specific activity (yes, I had planned this all along!). ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you could first download the names of all the rappers, starting from \n",
    "\n",
    "* https://en.wikipedia.org/wiki/Category:West_Coast_hip_hop_musicians\n",
    "* https://en.wikipedia.org/wiki/Category:East_Coast_hip_hop_musicians\n",
    "\n",
    "But this might result in so much pain and suffering that I will not make you do that (although you are very much welcome to try!). Instead, you can download all the names, nice and clean, here (it might still include couple of *noisy* links, but should be fine in 95% of records):\n",
    " \n",
    "* **[West coast List](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/WestCoastRappers.csv)**\n",
    "* **[East coast List](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/EastCoastRappers.csv)**\n",
    "\n",
    "The files contain the wiki-link of all rappers in the two lists above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part B: Building the networks\n",
    "\n",
    "Now, we're going to build one huge NetworkX directed graph, which includes both West-coast and East-coast rappers. \n",
    "\n",
    "The nodes in the network will be all the rappers, and we will place an edge between nodes $A$ and $B$ if the Wikipedia page of node $A$ links to the Wikipedia page of node $B$.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "> ***Video instructions:*** Getting started with rap music. Link: https://www.dropbox.com/scl/fi/ivd99y7tfeqpzj9lxgh0p/GettingStartedWithRapMusic.mp4?rlkey=6y3ye8iex6ogcy93jzyviqlej&dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> \n",
    "> _Exercise_: Build the network of rappers \n",
    "\n",
    "> Now we can build the network. Isn't this a little bit cool? What a dataset :)\n",
    "\n",
    "> The overall strategy for this is the following: \n",
    "> Take the pages you have downloaded for each rappers. \n",
    "> Each page corresponds to a rapper, which is a node in your network. \n",
    "> Find all the hyperlinks in a rapper's page that link to another node of the network (e.g. an other character). \n",
    "> There are many ways to do this, but below, I've tried to break it down into natural steps. \n",
    "> Keep in mind that the network should include **both** West-coast and East-coast rappers (and that it is possible that some West-coast rappers will have links to East-coast rappers and vice-versa).\n",
    "> \n",
    "> **Note**: When you add a node to the network, also include an `attribute` (i.e. that specifies the universe where the character comes from; either West coast, or East coast)\n",
    ">\n",
    ">\n",
    "> * Use a regular expression to extract all outgoing links from each of the pages you downloaded above. \n",
    "> * For each link you extract, check if the target is a rapper. If yes, keep it. If no, discard it.\n",
    "> * Use a NetworkX [`DiGraph`](https://networkx.github.io/documentation/development/reference/classes.digraph.html) to store the network. Store also the properties of the nodes (i.e. which coast they represent).\n",
    "> * When have you finished, you'll notice that some nodes do not have any out- or in- degrees. You may *discard* those from the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> *Exercise*: Simple network statistics and analysis\n",
    "\n",
    "> * What is the number of nodes in the network? \n",
    "> * More importantly, what is the number of links?\n",
    "> * What is the number of links connecting West coast and East coast? What do those links mean?\n",
    "> * Plot the in and out-degree distributions. What do you observe? Can you explain why the in-degree distribution is different from the out-degree distribution?\n",
    ">     * Compare the degree distribution to a *random network* with the same number of nodes and *p*\n",
    ">     * Compare the degree distribution to a *scale-free* network with the same number of nodes.\n",
    "> * Who are top 10 most connected rappers? (Report results for in-degrees and out-degrees). Comment on your findings. Is this what you would have expected?\n",
    "> * Who are the top 5 most connected West coast rappers (again in terms of both in/out-degree)?\n",
    "> * Who are the top 5 most connected East coast rappers (again in terms of both in/out-degree)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The total degree distribution (in + out degree) for you network should resemble the distribution displayed on the image below:\n",
    "![img](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/WestcoastvsEastcoast_degrees.png?raw=true)\n",
    "![img](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/WestcoastvsEastcoast_degrees_loglog.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
