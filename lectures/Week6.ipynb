{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We're now switching focus away from Network Science (for a little bit), beginning to think about _Natural Language Processing_ instead. In other words, today will be all about teaching your computer to \"understand\" text. This ties in nicely with our work on the rappers network, since it is built on the rapper Wikipedia pages which contain pieces of text. We've looked at the network so far - now, let's see if we can include the text. Today is about \n",
    "\n",
    "* Installing the _natural language toolkit_ (NLTK) package and learning the basics of how it works (Chapter 1)\n",
    "* Figuring out how to make NLTK to work with other types of text (Chapter 2).\n",
    "\n",
    "> _Reading_\n",
    "> The reading for today is Natural Language Processing with Python (NLPP) Chapter 1 and 2. [It's free online](http://www.nltk.org/book/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: A little lecture\n",
    "\n",
    "> [**_Video Lecture_**](https://www.dropbox.com/scl/fi/vj2h6pekdl6y8yid070y5/NLP_Intro.mp4?rlkey=kajuxgfl413ql3q1qwdrqf9ok&dl=0). Today is all about ***working*** with NLTK, so not much lecturing - just a few words on NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Installing and the basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 NLTK Install tips \n",
    "\n",
    "Check to see if `nltk` is installed on your system by typing `import nltk` in a `notebook`. If it's not already installed, install it as part of _Anaconda_ by typing \n",
    "\n",
    "     conda install nltk \n",
    "\n",
    "at the command prompt. If you don't have them, you can download the various corpora using a command-line version of the downloader that runs in Python notebooks (as mentioned in Ch. 1 Sec. 1.2 of the book): \n",
    "\n",
    "In the Jupyter notebook, run the code \n",
    "\n",
    " `import nltk`\n",
    " \n",
    " `nltk.download()`\n",
    "\n",
    "Now you can hit `d` to download, then type \"book\" to fetch the collection needed today's `nltk` session. Now that everything is up and running, let's get to the actual exercises.\n",
    "\n",
    "**Note**: If, instead of opening the pop-up, your Kernel dies, you can direclty download the collection by typing `nltk.download(\"book\")` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Introduction to NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now that you have `nltk` installed, work through Chapter 1, Sections 1 to 4. The book is set up as a kind of tutorial with lots of examples for you to work through. I recommend you read the text with an open Jupyter Notebook and type out the examples that you see. ***It becomes much more fun if you add a few variations and see what happens***. We tweaked some of these examples in the following exercises and they might very well be due as assignments, so those ones should definitely be in a `notebook`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now that you have gone through Chapter 1, let's try some of the `nltk` functionalities on our own Text!\n",
    "\n",
    "> Setup. We want to start from a clean version of the rapper Wikipedia pages. A version with as little wiki-markup as possible. We needed it earlier to get the links, but now we want a readable version. To do this we need to create 10 new regex patterns and parse the pages ... \n",
    ">\n",
    "> No! Don't worry! We can get a fairly nice version directly from the Wikipedia API. Follow the instructions below:\n",
    ">\n",
    "> * read the [csv file](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/Rappers.csv) of the rappers with pandas `pd.read_csv()` and assign it to `df_rappers`, but change the parameters that you use to call the api to:      \n",
    "    - `\"action\": \"query\"`   \n",
    "    - `\"prop\": \"extracts\"`,\n",
    "    - `\"exlimit\":\"1\"`,\n",
    "    - `\"explaintext\": \"1\"`,\n",
    "    - `\"format\": \"json\"`,\n",
    "    - `\"titles\": [INSERT NAME OF THE RAPPER]` \n",
    "> * loop through the `Name` column of `df_rappers` and for each name make a request to the API\n",
    ">     * **Note1** if you are using the `urllib`, replace white spaces with `_` and use `urllib.parse.quote_plus(NAME_OF_RAPPER_HERE)` to build your title;\n",
    "> * after using `json` to load the text, remember to get the value (the clean text we are looking for) from the right key \n",
    ">     * **Note2** the key you are looking for is now `extract` and not `*`; check that your code gets the right page (you may have to do a manual exploration of the json structure again.\n",
    ">\n",
    "> Use this method to retrieve a nice copy of all rappers' text. Save each rapper's text in a txt file and collect them all in a new folder!\n",
    "\n",
    "> Finally, we can create our own corpus (see Ch. 2 Sec. 1.9 for more details):\n",
    ">    * create a list of file names `file_list` and order them with the same order in `df_rappers`, e.g., 03_Greedo.txt first, 22Gz.txt second, etc.\n",
    ">    * use the function `PlaintextCorpusReader` from `nltk.corpus` to create the rapper corpus.\n",
    ">    * use the function `nltk.Text()` as follows `nltk.Text(YOUR_CORPUS_HERE.words())`.\n",
    ">\n",
    "> Now you can use this nltk Text object as any other you've seen in the examples of Chapter 1!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exercises: NLPP Chapter 1 \n",
    "(the stuff that might be due in an upcoming assignment)\n",
    "\n",
    "The following exercises are from Chapter 1 but we are going to use them to study the rapper Wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 1*: Exploratory analysis\n",
    ">\n",
    "> * Try out the `concordance` method, using the rapper Wikipedia text and a word of your own choosing.\n",
    "> * Also try out the `similar` and `common_contexts` methods, again with any words you like.\n",
    "> * Create the rapper Wikipedia version of a dispersion plot:\n",
    ">    1. try your own version of the dispersion plot (use any set of words you like but remember to explain what you observe).\n",
    "> * What is a bigram? How does it relate to `collocations`. Explain in your own words.\n",
    ">    1. Compute the collocations on the rapper Wikipedia Text.\n",
    "> * Review the discussion of conditionals in Sec. 4. Find all words in the rapper Wikipedia Text starting with the letter *h*. Show the first 5 in alphabetical order. Try with a few other letters. Any fun examples of top-five words?\n",
    "> * Use `.index()` to find the index of the word *Snoop*. You'll need to insert this word as an argument between the parentheses. By a process of trial and error, find the slice for the complete sentence that contains this word.\n",
    "> * Review the discussion of looping with conditions in Sec 4. Use a combination of `for` and `if` statements to loop over the words of the rapper Wikipedia text and print the 5 longest uppercase words, one per line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise 2*: Basic stats\n",
    ">\n",
    "> * How many tokens are there in the rapper Wikipedia text? How many distinct tokens are there?\n",
    "> * Explain in your own words what aspect of language _lexical diversity_ describes. \n",
    ">     1. Compute the lexical diversity of the rapper Wikipedia text;\n",
    ">     2. Compute the lexical diversity related to West (coast) and East (coast). Comment on your results.\n",
    "> * Create frequency distributions for the rapper Wikipedia text, including the cumulative frequency plot for the 75 most common tokens. \n",
    "> * Find all the four-letter tokens in the rapper Wikipedia text. With the help of a frequency distribution (FreqDist), show these tokens in decreasing order of frequency.\n",
    "> * What does the following code do? `sum(len(w) for w in rapperWiki)` Can you use it to work out the average token length of the rapper Wikipedia text?\n",
    "> * Define a function:\n",
    ">     1. called `vocab_size(text)` that has a single parameter for the `text`, and which returns the vocabulary size of the text. Apply it to the rapper Wikipedia text;\n",
    ">     2. `percent(word, text)` that calculates how often a given `word` occurs in a `text`, and expresses the result as a percentage. Apply it to the rapper Wikipedia Text to compute the percentage for *Dre*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Working with NLTK and other types of text\n",
    "\n",
    "So far, we've worked with text from the rapper Wikipedia. But that's not the only source of text in the universe. In fact, it's far from it. Chapter 2 in NLPP is all about getting access to nicely curated texts that you can find built into NLTK.\n",
    "> \n",
    "> Reading: NLPP Chapter 2, Sec. 1 - 4.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercises*: NLPP Chapter 2\\. (other stuff that might be due in an assignment)\n",
    "> \n",
    "> * Solve exercise 4, 8, 11, 15, 16, 17, 18 in NLPP, section 2.8\\. As always, I recommend you write up your solutions nicely in a `notebook`.\n",
    "> * Work through exercise 2.8.23 on Zipf's law. [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) connects to a property of the Barabasi-Albert networks. Which one? Take a look at [this article](https://www.hpl.hp.com/research/idl/papers/ranking/adamicglottometrics.pdf) and write a paragraph or two describing other important instances of power-laws found on the internet.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
