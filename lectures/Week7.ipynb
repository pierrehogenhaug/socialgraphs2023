{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This week's curriculum is a little bit of a mixed bag. We'll cover 2 topics that are not super-related, but both are _very useful_ (and there is some connection between them). The overview is\n",
    "\n",
    "* Tricks for raw text (NLPP, Chapter 3) and finding the important words in a document (TF-IDF)\n",
    "* Community Detection\n",
    "\n",
    "In the first part, we will take a quick tour of NLPP's chapter 3, which is boring, but an amazing ressource that you'll keep returning to. Then we'll talk about how we can use simple statistics & machine learning to get text to show us what it's all about. We will even do a little visualization. \n",
    "\n",
    "In the second part we will go back to network science, discussing community detection and trying it out on our very own dataset.\n",
    "\n",
    "## The informal intro\n",
    "\n",
    "You didn't think you'd be able to avoid hearing a little update from me, did you? I didn't think so :) \n",
    "* Today, I'll go over the work we'll be focusing on today, then\n",
    "* And finally, I'll talk a tiny amount about the next phase of the class (the project assignments).\n",
    "\n",
    "(Next week, however, the informal intro will provide full details about the project phase of the class, so that's one not to miss.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Video** [A few comments about today + the final project](https://www.dropbox.com/scl/fi/1l8jbtye91705f90sq1w0/Week7.mp4?rlkey=h0fwvdpetaaf33bz2it28d6s1&dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing real text (from out on the inter-webs)\n",
    "\n",
    "Ok. So Chapter 3 in NLPP is all about working with text from the real world. Getting text from this internet, cleaning it, tokenizing, modifying (e.g. stemming, converting to lower case, etc) to get the text in shape to work with the NLTK tools you've already learned about - and many more. \n",
    "\n",
    "In the process we'll learn more about regular expressions, as well as unicode; something we've already been struggling with a little bit will now be explained in more detail. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Reading*: NLPP Chapter 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.9, and 3.10\\. It's not important that you go in depth with everything here - the key think is that you *know that Chapter 3 of this book exists*, and that it's a great place to return to if you're ever in need of an explanation of regular expressions, unicode, or other topics that you forget as soon as you stop using them (and don't worry, I forget about those things too).\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words that characterize the rappers\n",
    "\n",
    "In this section, we'll begin to play around with how far we can get with simple strategies for looking at text. \n",
    "\n",
    "The video is basically just me talking about a fun paper, which shows you how little is needed in order to reveal something highly interesting about humans that produce text. But it's important. Don't miss this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Video lecture**: Simple methods reveal a lot. I talk a little bit about one of my favorite papers: [A mystery case to solve](https://www.dropbox.com/scl/fi/4bumpw918ez2de38k12w2/Binongo.mp4?rlkey=9wxe6kbvo9mr09bzyyxb1pzjl&dl=0).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use the wordcloud visualization techniques from the paper/video to learn about the wiki-pages we've downloaded. \n",
    "\n",
    "In the exercises below, we'll use a technique from Sune Lehmann's very own brain to do the West coast vs. East coast comparison (he calls call it \"TF-TR\"). Then proceed to some network analysis (!) to find network communities among rappers from one of the coasts. Finally, jump back to NLP to use the communities we've just found to play with TF-IDF, a more standard technique. \n",
    "\n",
    "We'll also do a bit of data cleaning along the way (using some of the stuff you've just read about in Chapter 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise_ 1: Comparing word-counts of West-coast and East-coast rappers. \\[The longest exercise in the class ðŸ˜±\\]. It consists of four steps. And it's great.\n",
    "\n",
    "\n",
    "**Step one: TF List**\n",
    "\n",
    "The goal of this exercise is to create your own wordclouds, characterizing the two coasts. Check out my awesome word-clouds below. I think it's pretty clear\n",
    "\n",
    "East coast            |  West coast\n",
    ":-------------------------:|:-------------------------:\n",
    "![im](https://raw.githubusercontent.com/SocialComplexityLab/socialgraphs2023/master/files/wordcloud_east.png \"east\")  |  ![im](https://raw.githubusercontent.com/SocialComplexityLab/socialgraphs2023/master/files/wordcloud_west.png \"west\")\n",
    "\n",
    "\n",
    "*Setup*. All you need now is the wikipedia pages of the rappers. If you didn't keep the wiki pages you previously downloaded, you can find them [here](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/eastcoasttexts.zip) and [here](https://github.com/SocialComplexityLab/socialgraphs2023/blob/main/files/westcoasttexts.zip).\n",
    "\n",
    "Once you have the texts down on your own computer, you will want to aggregate the text into two long lists. One based on all the text from the West coast pages, and one based on all the text from the East coast pages. In each list, you should keep all the words (or *tokens* to be more precise) that occur on the pages, and a count of how frequently each word occurs. For example, my West coast list contains the entries:\n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "dre 578\n",
    "snoop 424\n",
    "california 458\n",
    "\n",
    "...\n",
    "```\n",
    "This list is called a ***Term Frequency*** (or TF) list. Let's build our own TF lists. Before you start counting, I am going to ask you do do a few things\n",
    "\n",
    "> *Action items*\n",
    "> * Tokenize the pages into individual strings\n",
    "> * Remove all punctuation from your list of tokens\n",
    "> * Set everything to lower case\n",
    "> * (Optional) Lemmatize your words\n",
    "\n",
    "If you are confused by the instructions for cleaning, go back and have a  look Chapter 3 again. \n",
    "\n",
    "> *Action item*: \n",
    "> * Create your TF list for each coast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step two: Word weights**\n",
    "\n",
    "TF is not necessarily a good way of sorting a list, since many words are very common, so the most common words are not necessarily the most important ones. This is clear from the top counts in my west coast TF list\n",
    "\n",
    "```\n",
    "the 17309\n",
    "and 9778\n",
    "in 8237\n",
    "a 6985\n",
    "of 6434\n",
    "on 6341\n",
    "to 5107\n",
    "his 4619\n",
    "he 4325\n",
    "was 4266\n",
    "\n",
    " ```\n",
    " \n",
    "You can fix some of this by removing stop-words (as is described in the book), but it's not enough. ***We want to pull out what's important.*** Thus, to create awesome and informative wordclouds like the ones I created above, we want to extract what's special about each of the two lists we're comparing. \n",
    "\n",
    "The general way of doing that is using a strategy called TF-IDF. We will explore that strategy in exercise 4 below. But out-of-the-box TF-IDF gets a bit weird when we only have two groups to compare. So we'll do something different (but related) here. \n",
    "\n",
    "We want to use the information stored in TF, which is important since it says something about the most frequently occuring words. **But we want to weigh the TF information** using additional information about what is unique about each coast. \n",
    "\n",
    "Specifically, we want to set things up such that - in the west-coast list, for example - words that are unique to the West Coast get a high weight, while words that are unique to the East Coast get a low weight, and words that occur at the same frequency in both lists are neutral. Vice versa for the west-coast list.\n",
    "\n",
    "The way I chose to set up the weights here is simple. I use term ratios (TR), which I just made up, so don't look it up on the internet. What we care about is words that are used very differently for the two coasts, so we just use their ratios. \n",
    "\n",
    "**normalization..** Let me start with an example. The word `california` occurs 458 in the West-coast TF list and 87 times in the East-coast TF list, thus I set it's West-coast weight to\n",
    "\n",
    "$$w_\\textrm{california}^{(m)} = \\frac{458}{87 + c} = 4.28.$$\n",
    "\n",
    "Similarly, its East-coast weight is \n",
    "\n",
    "$$w_\\textrm{california}^{(d)} = \\frac{87}{458 + c} = 0.18.$$\n",
    "\n",
    "In both cases, I add the constant $c$ to the denominator in case a word occurs zero times. You can play around with the size of $c$ to understand the effect of chosing small/large values of it.\n",
    "\n",
    "In general for some token $t$ with term frequency $\\textrm{TF}^{(u)}_t$, in coast $u$ where $u \\in \\{w,e\\}$, we define the weight as:\n",
    "\n",
    "$$w_{t}^{(w)} = \\frac{\\textrm{TF}^{(w)}_t}{ \\textrm{TF}^{(e)}_t + c}, \\qquad \\textrm{and} \\qquad w_{t}^{(e)} = \\frac{\\textrm{TF}^{(e)}_t}{ \\textrm{TF}^{(w)}_t + c}. $$\n",
    "\n",
    "Thus, now we're ready to lists for each univers, where the ranking of token $t$ on list $u$ is given by $\\textrm{TF}^{(u)}_t \\times w_{t}^{(u)}$. I call this the *TF-TR* lists. \n",
    "\n",
    "Note that the *TF-TR* lists have the properties that we requested above. The weight associated with a word is large when a word occurs much more frequently in the list we consider, compared to the other list. It's small when a word is rare in our list and frequent in the other. And it's approximately equal to one, when the word is equally frequent in both lists. (The downside is that it only works when you're finding important terms while comparing two lists).\n",
    "\n",
    "> *Action item*: \n",
    "> * Create your TF-TR list for each coast.\n",
    "> * Check out top 10 for each coast. Does it make sense?\n",
    "\n",
    "The takehome here is that we get a good ranking by combining the term frequency with some weighting scheme. But, as we will see below, the TR weight that I created is not the only possible weight. There are many other options.\n",
    "\n",
    "-----------\n",
    "\n",
    "PS. Above in creating the ratios, I'm assuming that the coast pages have a comparable number of words in them. That's pretty much true (up to a factor of 2). And while the approximation is OK for our purposes, it's not true in general. If you'd like an extra challenge, you can figure out how to account for differences in the size of each coast corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step three: Install the software**\n",
    "\n",
    "First you must set up your system. The most difficult thing about creating the wordcloud is installing the `WordCloud` module. It's available on GitHub, check out the page [**here**](https://github.com/amueller/word_cloud). \n",
    "\n",
    "If you're lucky, you can simply install using conda (and all dependencies, etc will be automatically fixed): \n",
    "\n",
    "    conda install -c conda-forge wordcloud\n",
    " \n",
    "If you can't get that to work you can refer here https://anaconda.org/conda-forge/wordcloud. \n",
    "Also, maybe the comments below are helpful: \n",
    " * The module depends on the Python library PIL. Use `conda` to install that before you do anything else.\n",
    " * On my system, the module needed the `gcc` compiler installed. If you're not already a programmer, you may have to install that. On Mac you get it by installing the [_command line tools_](http://osxdaily.com/2014/02/12/install-command-line-tools-mac-os-x/). On linux, it's probably already installed. And on Windows I'm not sure, but we'll figure it out during the exercises. \n",
    " * Once that's all set up, you can use `pip` to install the `WordCloud` library, as [detailed on the GitHub page](https://github.com/amueller/word_cloud). But don't forget to use Anaconda's `pip` just as when you installed the communities library a few weeks ago. \n",
    " * There are examples of how to use the module online, see [here](http://sebastianraschka.com/Articles/2014_twitter_wordcloud.html) and [here](https://bioinfoexpert.com/2015/05/26/generating-word-clouds-in-python/). If you're interested, you can read about how the package was put together [here](http://peekaboo-vision.blogspot.dk/2012/11/a-wordcloud-in-python.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step four: Draw the wordclouds**\n",
    "\n",
    "> *Action items*\n",
    "> * Get your lists ready for the word-cloud software\n",
    ">    - The package needs a single string to work on. The way that I converted my lists to a string was to simply combine all words together in one long string (separated by spaces), repeating each word according to its score (rounded up to the nearest integer value). \n",
    ">    - The `wordcloud` package looks for collocations in real texts, which is a problem when you make the list as above. The recommended fix is to simply set `collocations = False` as an option when you run the package.\n",
    "> * Now, create a word-cloud for each cost. Feel free to make it as fancy or non-fancy as you like. Comment on the results. (If you'd like, you can remove stopwords/wiki-syntax - I did a bit of that for my own wordclouds.)\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have worked hard on text analysis, it is time to go back to our network! \n",
    "\n",
    "Before we begin, let's watch a great lecture to learn about communities. If you want all the details, I recommend you take a look at _Chapter 9_ in the _Network Science_ book ... but the video lecture below should be enough to get you started. \n",
    "\n",
    "**Note**: For this and the next exercise, work on the _undirected_ version of the network.\n",
    "\n",
    "> **_Video Lecture_**: [Communities in networks](https://www.dropbox.com/scl/fi/jw6prey62yyc0cpjnk4nv/Communities.mp4?rlkey=62lulz7b238rmiuaenbm8ts8t&dl=0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 2*: Use the concept of modularity to explore how _community-like_ the coasts are.\n",
    "> \n",
    "> * Explain the concept of modularity in your own words.\n",
    "> * Consider the undirected version of the entire network, including both coasts.\n",
    "> * Now create your own partition into communities, where all west-coast rappers are one community and all the east-coast rappers are another community. That's all you need, **now calculate the modularity of this partition**. Modularity is described in the _Network Science_ book, section 9.4.. Thus, use **equation 9.12** in the book to calculate the modularity _M_ of the partition described above. Are the coasts good communities?\n",
    "> * Would you expect this result in light of what we have found in the previous exercises?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 3*: Community detection, considering each coast separately. \n",
    "> * Consider the network of rappers for each coast separately. \n",
    ">    - **Note**. For this exercise, it's OK to just pick one of the coasts. \n",
    ">    - If you want to work with the entire network. Then run community detection on each coast separately, then combine results to get a list containing all communities from both parts of the network.\n",
    ">    - (The reason for this is that the strong split between the coasts can confuse the algorithm a bit).\n",
    "> * Use [the Python Louvain-algorithm implementation](http://perso.crans.org/aynaud/communities/) to find communities in each network. Report the value of modularity found by the algorithm. Is it higher or lower than what you found above for the coasts as communities? What does this comparison reveal about them?\n",
    "    >   * **Note**: This implementation is also available as Anaconda package. Install with `conda` as expained [here](https://anaconda.org/auto/python-louvain). \n",
    "    >   * You can also try the *Infomap* algorithm instead if you're curious. Go to [this page]. (http://www.mapequation.org/code.html) and search for 'python'. It's harder to install, but a better community detection algorithm.\n",
    "> Visualize the network, using the Force Atlas algorithm (see Lecture 5, exercise 2). This time assign each node a different color based on their _community_. Describe the structure you observe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we jump back into the NLP work. **It's still OK to work with just one coast**. In this last exercise, we'll be creating word-clouds again. But this time, we'll be using the more standard method: TF-IDF.\n",
    "\n",
    "\n",
    "*Exercise 4*: Wrap your brain around TF-IDF\n",
    "\n",
    "\n",
    "\n",
    "First, let's learn about TF-IDF the way wikipedia explains it. Check out [the wikipedia page for TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and ***read the first part very carefully***. The idea is still to have a *term frequency* (TF) and a *weight* (IDF), but there are some additional things going on. For example, there are multiple definitions of TF. We just looked at the raw counts.\n",
    "\n",
    "> *Action items*\n",
    ">   * Pick one of the alternative term frequency definitions. Explain why it might sometimes be prefereable to the raw count.\n",
    ">   * What does IDF stand for?\n",
    "\n",
    "There are also multiple versions of IDF. Let's think about those for a moment.\n",
    "\n",
    "> *Action items*\n",
    ">   * All of the IDF versions take the log of the calculated weight. Why do you think that is?\n",
    ">   * Explain why using IDF makes stopword removal less important.\n",
    ">   * In the TR weight that I defined in Exercise 1, we take into account how frequently each word appears inside each of the two documents. Is information of word counts inside each document used in the definition of IDF on Wikipedia?\n",
    "\n",
    "I noted above that out-of-the box worked weirdly when you only have two documents. Let's see why that's the case. If we grab one of the simple definitions of IDF-weight from wikipedia\n",
    "\n",
    "$$\\log \\left( \\frac{N}{n_t+1} \\right) + 1.$$\n",
    "\n",
    "Where $N = 2$ is the number of documents and $n_t \\in \\{1,2\\}$ is the number of documents containing the term $t$.\n",
    "\n",
    "> *Action item*\n",
    ">   * What are the possible weights that a word can have?\n",
    ">   * Explain in your own words why TF-IDF might not result in ideal wordclouds when you only have two documents.\n",
    "\n",
    "*Pro-level consideration*: It is, of course, possible to define IDF weighting schemes that incorporate information of word counts within each document, even if you have more than two documents. If you'd like to try to do that below, it's OK with me. If not, that's also fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To be continued ...**. Next week, we'll look at sentiment and TFIDF for communities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
